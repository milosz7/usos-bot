{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-28T19:48:21.075084Z",
     "start_time": "2024-10-28T19:48:20.312962Z"
    }
   },
   "source": [
    "from USOSDataLoader import USOSDataLoader\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from uuid import uuid4\n",
    "import logging\n",
    "import math\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:49:43.225908Z",
     "start_time": "2024-10-28T19:48:21.076004Z"
    }
   },
   "cell_type": "code",
   "source": "documents = USOSDataLoader().get_documents()",
   "id": "846bc6e7cd123d0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pdf links...: 100%|██████████| 34/34 [00:38<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading web data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pdf data...: 100%|██████████| 47/47 [00:29<00:00,  1.62it/s]\n",
      "Preprocessing documents...: 100%|██████████| 81/81 [00:00<00:00, 3720.39it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:49:51.903785Z",
     "start_time": "2024-10-28T19:49:43.226613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename=\"failed_chunks.log\", level=logging.ERROR)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v3\",\n",
    "                                   model_kwargs={\"trust_remote_code\": True},\n",
    "                                   encode_kwargs={\"task\": \"retrieval.query\"})\n",
    "\n",
    "pc = Pinecone(os.environ.get(\"PINECONE_API_KEY\"))"
   ],
   "id": "32fc07d1e47bf2c8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:49:52.423230Z",
     "start_time": "2024-10-28T19:49:51.904950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index_name = \"usos-bot-questions\"\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vectorstore = PineconeVectorStore(index=index, embedding=embeddings)"
   ],
   "id": "e64eeba7283378ed",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:49:52.432186Z",
     "start_time": "2024-10-28T19:49:52.424639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QuestionList(BaseModel):\n",
    "    question_list: list[str] = Field(..., title=\"List of questions generated for the document or fragment\")\n",
    "\n",
    "\n",
    "def clean_and_filter_questions(questions: list[str]) -> list[str]:\n",
    "    cleaned_questions = []\n",
    "    for question in questions:\n",
    "        cleaned_question = re.sub(r'^\\d+\\.\\s*', '', question.strip())\n",
    "        if cleaned_question.endswith('?'):\n",
    "            cleaned_questions.append(cleaned_question)\n",
    "    return cleaned_questions\n",
    "\n",
    "\n",
    "def llm_chain(llm, text, n_questions, prompt):\n",
    "    chain = prompt | llm.with_structured_output(QuestionList)\n",
    "    input_data = {\"context\": text, \"num_questions\": n_questions}\n",
    "    result = chain.invoke(input_data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_questions(text: str, n_questions) -> list[str]:\n",
    "    llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"num_questions\"],\n",
    "        template=\"Using the context data: {context}\\n\\nGenerate a list of at least {num_questions} \"\n",
    "                 \"possible questions that can be asked about this context. Ensure the questions are \"\n",
    "                 \"directly answerable within the context and do not include any answers or headers. \"\n",
    "                 \"The questions should be in the same language as the context. \"\n",
    "                 \"Separate the questions with a new line character.\"\n",
    "    )\n",
    "\n",
    "    prompt_secondary = PromptTemplate(\n",
    "        input_variables=[\"context\", \"num_questions\"],\n",
    "        template=\"Using the context data: {context}\\n\\nGenerate a list of at least {num_questions} \"\n",
    "                 \"possible questions that can be asked about this context. Ensure the questions are \"\n",
    "                 \"directly answerable within the context and do not include any answers or headers. \"\n",
    "                 \"The questions should be in the Polish language. \"\n",
    "                 \"Separate the questions with a new line character.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = llm_chain(llm, text, n_questions, prompt)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logger.error(f\"FAILED CHUNK: {text}\")\n",
    "        \n",
    "        result = llm_chain(llm, text, n_questions, prompt_secondary)\n",
    "\n",
    "    questions = result.question_list\n",
    "\n",
    "    filtered_questions = clean_and_filter_questions(questions)\n",
    "    return list(set(filtered_questions))\n",
    "\n",
    "\n",
    "def split_document(document: str, chunk_size: int, chunk_overlap: int) -> list[str]:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', document)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - chunk_overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunks.append(chunk_tokens)\n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "    return [\" \".join(chunk) for chunk in chunks]\n",
    "\n",
    "\n",
    "def print_document(comment: str, document: Document) -> None:\n",
    "    print(\n",
    "        f'{comment} (type: {document.metadata[\"type\"]}, index: {document.metadata[\"index\"]}): {document.page_content}')"
   ],
   "id": "dc82e492b790d2e9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:54:32.432888Z",
     "start_time": "2024-10-28T19:54:32.429033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_sleep_time(raw_sleep):\n",
    "    SECONDS_IN_MINUTE = 60\n",
    "    mins_match = re.search(\"\\\\d+(?=m)\", raw_sleep)\n",
    "    if mins_match:\n",
    "        mins_match = int(mins_match.group(0))\n",
    "        \n",
    "    seconds_match = re.search(r\"\\d+\\.\\d+(?=s)\", raw_sleep)\n",
    "    if seconds_match:\n",
    "        seconds_match = math.ceil(float(seconds_match.group(0)))\n",
    "        \n",
    "    return SECONDS_IN_MINUTE * mins_match + seconds_match"
   ],
   "id": "c83e8592c8dcc529",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Routine for processing fragments",
   "id": "f96609e016e4e4c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:54:32.775937Z",
     "start_time": "2024-10-28T19:54:32.769829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_fragment(fragment, i, j, counter, source):\n",
    "    knowledge_base = [Document(\n",
    "        page_content=fragment,\n",
    "        metadata=dict(type=\"ORIGINAL\", index=counter, source=source, orig_text=fragment)\n",
    "    )]\n",
    "    questions = generate_questions(fragment, n_questions=20)\n",
    "    knowledge_base.extend([\n",
    "        Document(page_content=question,\n",
    "                 metadata=dict(type=\"AUGMENTED\", index=counter + idx, orig_text=fragment))\n",
    "        for idx, question in enumerate(questions)\n",
    "    ])\n",
    "    counter += len(questions)\n",
    "    print(f'Text document {i} Text fragment {j} - generated: {len(questions)} questions')\n",
    "\n",
    "    uuids = [str(uuid4()) for _ in range(len(knowledge_base))]\n",
    "    vectorstore.add_documents(documents=knowledge_base, ids=uuids)\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def process_fragments(documents: list[Document], chunk_size: int, chunk_overlap: int, counter: int = 0):\n",
    "    for i, document in enumerate(documents):\n",
    "        text = document.page_content\n",
    "        text_fragments = split_document(text, chunk_size, chunk_overlap)\n",
    "        for j, fragment in enumerate(text_fragments):\n",
    "            print(f\"Document {i} - split into {len(text_fragments)}\")\n",
    "            try:\n",
    "                counter = add_fragment(fragment, i, j, counter, source=document.metadata[\"source\"])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                match = re.search(r\"\\d+m\\d+\\.\\d+s|\\d+.\\ds|\\d+m\", e.message)\n",
    "                if match:\n",
    "                    match = match.group(0)\n",
    "                    sleep_parsed = parse_sleep_time(match)\n",
    "                    print(f\"Sleeping for {sleep_parsed} seconds...\")\n",
    "                    time.sleep(sleep_parsed)\n",
    "                # if no match - let it crash\n",
    "                counter = add_fragment(fragment, i, j, counter, source=document.metadata[\"source\"])"
   ],
   "id": "1e8e3dbd8f9d2ee3",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Routine for adding document by documents with logging",
   "id": "cb1593e733913553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_document(document, chunk_size, chunk_overlap, i, counter):\n",
    "    knowledge_base = []\n",
    "    text = document.page_content\n",
    "    text_fragments = split_document(text, chunk_size, chunk_overlap)\n",
    "    print(f\"Document {i} - split into {len(text_fragments)}\")\n",
    "    for j, fragment in enumerate(text_fragments):\n",
    "        knowledge_base.append(\n",
    "            Document(\n",
    "                page_content=fragment,\n",
    "                metadata=dict(type=\"ORIGINAL\", index=counter, source=document.metadata[\"source\"], text=fragment)\n",
    "            )\n",
    "        )\n",
    "        questions = generate_questions(text, n_questions=20)\n",
    "        knowledge_base.extend([\n",
    "            Document(page_content=question,\n",
    "                     metadata={\"type\": \"AUGMENTED\", \"index\": counter + idx, \"text\": fragment})\n",
    "            for idx, question in enumerate(questions)\n",
    "        ])\n",
    "        counter += len(questions)\n",
    "        print(f'Text document {i} Text fragment {j} - generated: {len(questions)} questions')\n",
    "    \n",
    "    uuids = [str(uuid4()) for _ in range(len(knowledge_base))]\n",
    "    vectorstore.add_documents(documents=knowledge_base, ids=uuids)\n",
    "\n",
    "\n",
    "def process_documents(documents: list[Document], chunk_size: int, chunk_overlap: int, counter: int = 0):\n",
    "    for i, document in enumerate(documents):\n",
    "        try:\n",
    "            counter = add_document(document, chunk_size, chunk_overlap, i, counter)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"FAILED TO PARSE: DOCUMENT: {i}, COUNTER: {counter} - also accessible in the database. MESSAGE: {e.message}\")"
   ],
   "id": "60cd2e7d86fc1a0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "process_fragments(documents, 1000, 200)",
   "id": "96b954dd42302b19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:52:14.198231Z",
     "start_time": "2024-10-28T10:52:14.198183Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "68d3f630900c59c5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
