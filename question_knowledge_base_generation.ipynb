{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-28T10:27:46.009788Z",
     "start_time": "2024-10-28T10:27:45.918330Z"
    }
   },
   "source": [
    "from USOSDataLoader import USOSDataLoader\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from uuid import uuid4\n",
    "import math\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:31:54.428349Z",
     "start_time": "2024-10-28T10:27:46.014191Z"
    }
   },
   "cell_type": "code",
   "source": "documents = USOSDataLoader().get_documents()",
   "id": "846bc6e7cd123d0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pdf links...: 100%|██████████| 34/34 [01:40<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading web data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pdf data...: 100%|██████████| 47/47 [00:27<00:00,  1.69it/s]\n",
      "Preprocessing documents...: 100%|██████████| 81/81 [00:00<00:00, 3708.61it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:32:07.882715Z",
     "start_time": "2024-10-28T10:31:54.429020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v3\",\n",
    "                                   model_kwargs={\"trust_remote_code\": True},\n",
    "                                   encode_kwargs={\"task\": \"retrieval.query\"})\n",
    "\n",
    "pc = Pinecone(os.environ.get(\"PINECONE_API_KEY\"))"
   ],
   "id": "32fc07d1e47bf2c8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:32:08.382092Z",
     "start_time": "2024-10-28T10:32:07.883660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index_name = \"usos-bot-questions\"\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vectorstore = PineconeVectorStore(index=index, embedding=embeddings)"
   ],
   "id": "e64eeba7283378ed",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:32:08.401214Z",
     "start_time": "2024-10-28T10:32:08.386008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QuestionList(BaseModel):\n",
    "    question_list: list[str] = Field(..., title=\"List of questions generated for the document or fragment\")\n",
    "\n",
    "\n",
    "def clean_and_filter_questions(questions: list[str]) -> list[str]:\n",
    "    cleaned_questions = []\n",
    "    for question in questions:\n",
    "        cleaned_question = re.sub(r'^\\d+\\.\\s*', '', question.strip())\n",
    "        if cleaned_question.endswith('?'):\n",
    "            cleaned_questions.append(cleaned_question)\n",
    "    return cleaned_questions\n",
    "\n",
    "\n",
    "def llm_chain(llm, text, n_questions, prompt):\n",
    "    chain = prompt | llm.with_structured_output(QuestionList)\n",
    "    input_data = {\"context\": text, \"num_questions\": n_questions}\n",
    "    result = chain.invoke(input_data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_questions(text: str, n_questions) -> list[str]:\n",
    "    llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"num_questions\"],\n",
    "        template=\"Using the context data: {context}\\n\\nGenerate a list of at least {num_questions} \"\n",
    "                 \"possible questions that can be asked about this context. Ensure the questions are \"\n",
    "                 \"directly answerable within the context and do not include any answers or headers. \"\n",
    "                 \"The questions should be in the same language as the context. \"\n",
    "                 \"Separate the questions with a new line character.\"\n",
    "    )\n",
    "\n",
    "    prompt_secondary = PromptTemplate(\n",
    "        input_variables=[\"context\", \"num_questions\"],\n",
    "        template=\"Using the context data: {context}\\n\\nGenerate a list of at least {num_questions} \"\n",
    "                 \"possible questions that can be asked about this context. Ensure the questions are \"\n",
    "                 \"directly answerable within the context and do not include any answers or headers. \"\n",
    "                 \"The questions should be in the Polish language. \"\n",
    "                 \"Separate the questions with a new line character.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = llm_chain(llm, text, n_questions, prompt)\n",
    "    except Exception as e:\n",
    "        result = llm_chain(llm, text, n_questions, prompt_secondary)\n",
    "\n",
    "    questions = result.question_list\n",
    "\n",
    "    filtered_questions = clean_and_filter_questions(questions)\n",
    "    return list(set(filtered_questions))\n",
    "\n",
    "\n",
    "def split_document(document: str, chunk_size: int, chunk_overlap: int) -> list[str]:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', document)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - chunk_overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunks.append(chunk_tokens)\n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "    return [\" \".join(chunk) for chunk in chunks]\n",
    "\n",
    "\n",
    "def print_document(comment: str, document: Document) -> None:\n",
    "    print(\n",
    "        f'{comment} (type: {document.metadata[\"type\"]}, index: {document.metadata[\"index\"]}): {document.page_content}')"
   ],
   "id": "dc82e492b790d2e9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T17:38:47.901311Z",
     "start_time": "2024-10-28T17:38:47.899049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_sleep_time(raw_sleep):\n",
    "    SECONDS_IN_MINUTE = 60\n",
    "    mins_match = re.search(\"\\\\d+(?=m)\", raw_sleep)\n",
    "    if mins_match:\n",
    "        mins_match = int(mins_match.group(0))\n",
    "        \n",
    "    seconds_match = re.search(r\"\\\\d+.\\\\d+(?=s)\", raw_sleep)\n",
    "    if seconds_match:\n",
    "        seconds_match = float(seconds_match.group(0))\n",
    "        \n",
    "    return SECONDS_IN_MINUTE * mins_match + seconds_match"
   ],
   "id": "c83e8592c8dcc529",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:32:08.406013Z",
     "start_time": "2024-10-28T10:32:08.402092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_document(fragment, i, j, counter, source):\n",
    "    knowledge_base = [Document(\n",
    "        page_content=fragment,\n",
    "        metadata=dict(type=\"ORIGINAL\", index=counter, source=source, orig_text=fragment)\n",
    "    )]\n",
    "    questions = generate_questions(fragment, n_questions=20)\n",
    "    print(\"SLEEPING AFTER QUESTION GENERATION\")\n",
    "    time.sleep(60)\n",
    "    knowledge_base.extend([\n",
    "        Document(page_content=question,\n",
    "                 metadata=dict(type=\"AUGMENTED\", index=counter + idx, orig_text=fragment))\n",
    "        for idx, question in enumerate(questions)\n",
    "    ])\n",
    "    counter += len(questions)\n",
    "    print(f'Text document {i} Text fragment {j} - generated: {len(questions)} questions')\n",
    "    for doc in knowledge_base:\n",
    "        print_document(\"Dataset\", doc)\n",
    "\n",
    "    uuids = [str(uuid4()) for _ in range(len(knowledge_base))]\n",
    "    vectorstore.add_documents(documents=knowledge_base, ids=uuids)\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def process_documents(documents: list[Document], chunk_size: int, chunk_overlap: int):\n",
    "    counter = 0\n",
    "    for i, document in enumerate(documents):\n",
    "        text = document.page_content\n",
    "        text_fragments = split_document(text, chunk_size, chunk_overlap)\n",
    "        for j, fragment in enumerate(text_fragments):\n",
    "            print(f\"Document {i} - split into {len(text_fragments)}\")\n",
    "            try:\n",
    "                counter = add_document(fragment, i, j, counter, source=document.metadata[\"source\"])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                match = re.search(r\"\\\\d+m\\\\d+.\\\\d+s|\\\\d+.\\\\ds|\\\\d+m\", e[\"error\"][\"message\"])\n",
    "                if match:\n",
    "                    match = match.group(0)\n",
    "                    sleep_parsed = parse_sleep_time(match)\n",
    "                    time.sleep(sleep_parsed)\n",
    "                # if no match - let it crash\n",
    "                counter = add_document(fragment, i, j, counter, source=document.metadata[\"source\"])"
   ],
   "id": "1e8e3dbd8f9d2ee3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:54:00.735584Z",
     "start_time": "2024-10-28T10:52:25.122883Z"
    }
   },
   "cell_type": "code",
   "source": "process_documents(documents[37:], 1000, 200)",
   "id": "96b954dd42302b19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 23 - split into 1\n",
      "Text document 23 Text fragment 0 - generated: 20 questions\n",
      "Dataset (type: ORIGINAL, index: 0): Instrukcja uzupełniania dokumentów tożsamości osoby w USOS Java Aby uzupełnić dokumenty tożsamości osoby należy wyjeść do formularza Osoby Osoby odnaleźć szukaną osobę a następnie przejść do zakładki dokumenty W zakładce mogą znaleźć się już wcześniej wprowadzone dokumenty aby dodać kolejny wpis należy kliknąć przycisk po prawej stronie Dodaj Uzupełniamy wymagane pola i klikamy ikonę dyskietki aby zapisać zmiany\n",
      "Dataset (type: AUGMENTED, index: 0): Jak zapisać zmiany w dokumentach tożsamości?\n",
      "Dataset (type: AUGMENTED, index: 1): Czy mogę wydrukować dokumenty tożsamości?\n",
      "Dataset (type: AUGMENTED, index: 2): Co się dzieje, gdy nie wypełnię wymaganych pól?\n",
      "Dataset (type: AUGMENTED, index: 3): Czy mogę zapisać dokumenty bez klikania ikony dyskietki?\n",
      "Dataset (type: AUGMENTED, index: 4): Co się dzieje, gdy kliknię przycisk Dodaj?\n",
      "Dataset (type: AUGMENTED, index: 5): Jak znaleźć szukaną osobę w formularzu Osoby?\n",
      "Dataset (type: AUGMENTED, index: 6): Co trzeba zrobić, aby dodać nowy dokument tożsamości?\n",
      "Dataset (type: AUGMENTED, index: 7): Jakie pola są wymagane, aby dodać nowy dokument?\n",
      "Dataset (type: AUGMENTED, index: 8): Jak znaleźć osobę, której dokumenty chcę uzupełnić?\n",
      "Dataset (type: AUGMENTED, index: 9): Jak usunąć dokument tożsamości?\n",
      "Dataset (type: AUGMENTED, index: 10): Jak sprawdzić, czy dokumenty są poprawnie wypełnione?\n",
      "Dataset (type: AUGMENTED, index: 11): Gdzie znajduje się zakładka dokumenty?\n",
      "Dataset (type: AUGMENTED, index: 12): Co to jest formularz Osoby?\n",
      "Dataset (type: AUGMENTED, index: 13): Czy mogę dodać wiele dokumentów jednocześnie?\n",
      "Dataset (type: AUGMENTED, index: 14): Jak uzupełnić dokumenty tożsamości osoby w USOS Java?\n",
      "Dataset (type: AUGMENTED, index: 15): Czy mogę edytować już istniejące dokumenty?\n",
      "Dataset (type: AUGMENTED, index: 16): Jak posortować dokumenty tożsamości?\n",
      "Dataset (type: AUGMENTED, index: 17): Czy mogę uzupełnić dokumenty tożsamości innej osoby?\n",
      "Dataset (type: AUGMENTED, index: 18): Jak wyświetlić listę wszystkich dokumentów tożsamości?\n",
      "Dataset (type: AUGMENTED, index: 19): Czy mogę filtrować dokumenty tożsamości?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 34\u001B[0m, in \u001B[0;36mprocess_documents\u001B[0;34m(documents, chunk_size, chunk_overlap, start)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 34\u001B[0m     counter \u001B[38;5;241m=\u001B[39m \u001B[43madd_document\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocument\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_overlap\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcounter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "Cell \u001B[0;32mIn[8], line 25\u001B[0m, in \u001B[0;36madd_document\u001B[0;34m(document, chunk_size, chunk_overlap, i, counter)\u001B[0m\n\u001B[1;32m     24\u001B[0m uuids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mstr\u001B[39m(uuid4()) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(knowledge_base))]\n\u001B[0;32m---> 25\u001B[0m \u001B[43mvectorstore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mknowledge_base\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muuids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m counter\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:287\u001B[0m, in \u001B[0;36mVectorStore.add_documents\u001B[0;34m(self, documents, **kwargs)\u001B[0m\n\u001B[1;32m    286\u001B[0m     metadatas \u001B[38;5;241m=\u001B[39m [doc\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[0;32m--> 287\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    288\u001B[0m msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`add_documents` and `add_texts` has not been implemented \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    290\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    291\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/langchain_pinecone/vectorstores.py:279\u001B[0m, in \u001B[0;36mPineconeVectorStore.add_texts\u001B[0;34m(self, texts, metadatas, ids, namespace, batch_size, embedding_chunk_size, async_req, id_prefix, **kwargs)\u001B[0m\n\u001B[1;32m    278\u001B[0m chunk_metadatas \u001B[38;5;241m=\u001B[39m metadatas[i : i \u001B[38;5;241m+\u001B[39m embedding_chunk_size]\n\u001B[0;32m--> 279\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_embedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk_texts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    280\u001B[0m vector_tuples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(chunk_ids, embeddings, chunk_metadatas)\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/langchain_huggingface/embeddings/huggingface.py:86\u001B[0m, in \u001B[0;36mHuggingFaceEmbeddings.embed_documents\u001B[0;34m(self, texts)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 86\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow_progress\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_kwargs\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m embeddings\u001B[38;5;241m.\u001B[39mtolist()\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:621\u001B[0m, in \u001B[0;36mSentenceTransformer.encode\u001B[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    620\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 621\u001B[0m     out_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:688\u001B[0m, in \u001B[0;36mSentenceTransformer.forward\u001B[0;34m(self, input, **kwargs)\u001B[0m\n\u001B[1;32m    687\u001B[0m     module_kwargs \u001B[38;5;241m=\u001B[39m {key: value \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m module_kwarg_keys}\n\u001B[0;32m--> 688\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodule_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    689\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/jinaai/jina-embeddings-v3/fa78e35d523dcda8d3b5212c7487cf70a4b277da/custom_st.py:143\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, features, task)\u001B[0m\n\u001B[1;32m    142\u001B[0m features\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprompt_length\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 143\u001B[0m output_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mauto_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mlora_arguments\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    144\u001B[0m output_tokens \u001B[38;5;241m=\u001B[39m output_states[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm-roberta-flash-implementation/12700ba4972d9e900313a85ae855f5a76fb9500e/modeling_lora.py:357\u001B[0m, in \u001B[0;36mXLMRobertaLoRA.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm-roberta-flash-implementation/12700ba4972d9e900313a85ae855f5a76fb9500e/modeling_xlm_roberta.py:684\u001B[0m, in \u001B[0;36mXLMRobertaModel.forward\u001B[0;34m(self, input_ids, position_ids, token_type_ids, attention_mask, masked_tokens_mask, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m    680\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    681\u001B[0m     return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m    682\u001B[0m )\n\u001B[0;32m--> 684\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    687\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[43m    \u001B[49m\u001B[43madapter_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madapter_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    689\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[38;5;66;03m# TD [2022-12:18]: Don't need to force residual in fp32\u001B[39;00m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;66;03m# BERT puts embedding LayerNorm before embedding dropout.\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm-roberta-flash-implementation/12700ba4972d9e900313a85ae855f5a76fb9500e/embedding.py:65\u001B[0m, in \u001B[0;36mXLMRobertaEmbeddings.forward\u001B[0;34m(self, input_ids, position_ids, token_type_ids, adapter_mask)\u001B[0m\n\u001B[1;32m     64\u001B[0m task_input_ids \u001B[38;5;241m=\u001B[39m input_ids[task_indices]\n\u001B[0;32m---> 65\u001B[0m task_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask_input_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m embeddings[task_indices] \u001B[38;5;241m=\u001B[39m task_embeddings\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/usos-bot/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm-roberta-flash-implementation/12700ba4972d9e900313a85ae855f5a76fb9500e/modeling_lora.py:226\u001B[0m, in \u001B[0;36mLoRAParametrization.add_to_layer.<locals>.new_forward\u001B[0;34m(self, input, task_id)\u001B[0m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m task_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 226\u001B[0m     weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparametrizations\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlora_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcurrent_task\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask_id\u001B[49m\n\u001B[1;32m    228\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm-roberta-flash-implementation/12700ba4972d9e900313a85ae855f5a76fb9500e/modeling_lora.py:108\u001B[0m, in \u001B[0;36mLoRAParametrization.lora_forward\u001B[0;34m(self, X, current_task)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlora_forward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, current_task):\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    107\u001B[0m         X\n\u001B[0;32m--> 108\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mswap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m                \u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlora_B\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcurrent_task\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlora_A\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcurrent_task\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[43m                \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mview(X\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m    116\u001B[0m         \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaling\n\u001B[1;32m    117\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 17.55 GB, other allocations: 363.20 MB, max allowed: 18.13 GB). Tried to allocate 976.57 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mprocess_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m37\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m23\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[8], line 36\u001B[0m, in \u001B[0;36mprocess_documents\u001B[0;34m(documents, chunk_size, chunk_overlap, start)\u001B[0m\n\u001B[1;32m     34\u001B[0m     counter \u001B[38;5;241m=\u001B[39m add_document(document, chunk_size, chunk_overlap, i\u001B[38;5;241m+\u001B[39mstart, counter)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m---> 36\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m120\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m     counter \u001B[38;5;241m=\u001B[39m add_document(document, chunk_size, chunk_overlap, i\u001B[38;5;241m+\u001B[39mstart, counter)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T10:52:14.198231Z",
     "start_time": "2024-10-28T10:52:14.198183Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "68d3f630900c59c5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
