{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-27T18:28:08.033701Z",
     "start_time": "2024-10-27T18:28:08.026914Z"
    }
   },
   "source": [
    "from USOSDataLoader import USOSDataLoader\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from uuid import uuid4\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T18:08:39.537549Z",
     "start_time": "2024-10-27T18:07:14.848987Z"
    }
   },
   "cell_type": "code",
   "source": "documents = USOSDataLoader().get_documents()",
   "id": "846bc6e7cd123d0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pdf links...: 100%|██████████| 34/34 [00:39<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading web data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pdf data...: 100%|██████████| 47/47 [00:30<00:00,  1.56it/s]\n",
      "Preprocessing documents...: 100%|██████████| 81/81 [00:00<00:00, 3559.52it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T18:26:52.417868Z",
     "start_time": "2024-10-27T18:26:41.777042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v3\",\n",
    "                                   model_kwargs={\"trust_remote_code\": True},\n",
    "                                   encode_kwargs={\"task\": \"retrieval.query\"})\n",
    "\n",
    "pc = Pinecone(os.environ.get(\"PINECONE_API_KEY\"))"
   ],
   "id": "32fc07d1e47bf2c8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T18:27:35.637384Z",
     "start_time": "2024-10-27T18:27:34.868067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index_name = \"usos-bot-questions\"\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vectorstore = PineconeVectorStore(index=index, embedding=embeddings)"
   ],
   "id": "e64eeba7283378ed",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T18:28:34.222199Z",
     "start_time": "2024-10-27T18:28:34.204922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QuestionList(BaseModel):\n",
    "    question_list: list[str] = Field(..., title=\"List of questions generated for the document or fragment\")\n",
    "\n",
    "\n",
    "def clean_and_filter_questions(questions: list[str]) -> list[str]:\n",
    "    cleaned_questions = []\n",
    "    for question in questions:\n",
    "        cleaned_question = re.sub(r'^\\d+\\.\\s*', '', question.strip())\n",
    "        if cleaned_question.endswith('?'):\n",
    "            cleaned_questions.append(cleaned_question)\n",
    "    return cleaned_questions\n",
    "\n",
    "\n",
    "def generate_questions(text: str, n_questions) -> list[str]:\n",
    "    llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"num_questions\"],\n",
    "        template=\"Using the context data: {context}\\n\\nGenerate a list of at least {num_questions} \"\n",
    "                 \"possible questions that can be asked about this context. Ensure the questions are \"\n",
    "                 \"directly answerable within the context and do not include any answers or headers. \"\n",
    "                 \"The questions should be in the same language as the context. \"\n",
    "                 \"Separate the questions with a new line character.\"\n",
    "    )\n",
    "    chain = prompt | llm.with_structured_output(QuestionList)\n",
    "    input_data = {\"context\": text, \"num_questions\": n_questions}\n",
    "    result = chain.invoke(input_data)\n",
    "\n",
    "    questions = result.question_list\n",
    "\n",
    "    filtered_questions = clean_and_filter_questions(questions)\n",
    "    return list(set(filtered_questions))\n",
    "\n",
    "\n",
    "def split_document(document: str, chunk_size: int, chunk_overlap: int) -> list[str]:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', document)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - chunk_overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunks.append(chunk_tokens)\n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "    return [\" \".join(chunk) for chunk in chunks]\n",
    "\n",
    "def print_document(comment: str, document: Document) -> None:\n",
    "    print(f'{comment} (type: {document.metadata[\"type\"]}, index: {document.metadata[\"index\"]}): {document.page_content}')"
   ],
   "id": "dc82e492b790d2e9",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T18:45:50.599900Z",
     "start_time": "2024-10-27T18:45:50.589023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_document(document, chunk_size, chunk_overlap, i, counter):\n",
    "    knowledge_base = []\n",
    "    text = document.page_content\n",
    "    text_fragments = split_document(text, chunk_size, chunk_overlap)\n",
    "    print(f\"Document {i} - split into {len(text_fragments)}\")\n",
    "    for j, fragment in enumerate(text_fragments):\n",
    "        knowledge_base.append(\n",
    "            Document(\n",
    "                page_content=fragment,\n",
    "                metadata=dict(type=\"ORIGINAL\", index=counter, source=document.metadata[\"source\"], text=fragment)\n",
    "            )\n",
    "        )\n",
    "        questions = generate_questions(text, n_questions=20)\n",
    "        knowledge_base.extend([\n",
    "            Document(page_content=question,\n",
    "                     metadata={\"type\": \"AUGMENTED\", \"index\": counter + idx, \"text\": fragment})\n",
    "            for idx, question in enumerate(questions)\n",
    "        ])\n",
    "        counter += len(questions)\n",
    "        print(f'Text document {i} Text fragment {j} - generated: {len(questions)} questions')\n",
    "    for doc in knowledge_base:\n",
    "        print_document(\"Dataset\", doc)\n",
    "    \n",
    "    uuids = [str(uuid4()) for _ in range(len(knowledge_base))]\n",
    "    vectorstore.add_documents(documents=knowledge_base, ids=uuids)\n",
    "\n",
    "\n",
    "def process_documents(documents: list[Document], chunk_size: int, chunk_overlap: int):\n",
    "    counter = 0\n",
    "    for i, document in enumerate(documents):\n",
    "        try:\n",
    "            counter = add_document(document, chunk_size, chunk_overlap, i, counter)\n",
    "        except Exception as e:\n",
    "            time.sleep(120)\n",
    "            counter = add_document(document, chunk_size, chunk_overlap, i, counter)"
   ],
   "id": "1e8e3dbd8f9d2ee3",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-27T18:45:52.921362Z"
    }
   },
   "cell_type": "code",
   "source": "process_documents(documents, 1000, 200)",
   "id": "96b954dd42302b19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 - split into 2\n",
      "Text document 0 Text fragment 0 - generated: 21 questions\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "68d3f630900c59c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
